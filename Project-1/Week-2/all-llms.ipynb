{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4487f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.scrapper import fetch_website_contents, fetch_website_links\n",
    "from IPython.display import Markdown, display, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0ab3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AI\n",
      "OpenAI API Key exists and begins sk-proj-\n",
      "Groq API key exists and begins gsk\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API key exists and begins {groq_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1526d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "openai = OpenAI()\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "groq_url=\"https://api.groq.com/openai/v1\"\n",
    "ollama = OpenAI(api_key=\"Ollama\", base_url=ollama_url)\n",
    "gemini = OpenAI(base_url=gemini_url, api_key=google_api_key)\n",
    "groq = OpenAI(base_url=groq_url, api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bb8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "157a9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ollama = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=tell_a_joke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24dba1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_gemini = gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=tell_a_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc539536",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_groq = groq.chat.completions.create(model=\"openai/gpt-oss-20b\", messages=tell_a_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eef0ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa14a3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Did you know that the inventor of the Pringles potato‑chip can is literally part of the snack? Fred Baur, who came up with the iconic stack‑able chip container, died in 2008. His family honored him by burying part of his cremated remains inside a Pringles can, and he’s now “resting” in a snack‑wrapped grave!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_ollama.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71328070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their chatbot?\n",
       "\n",
       "Because every time they asked \"How are you feeling?\", it generated a 500-word essay on the philosophical implications of consciousness in artificial intelligence, complete with citations, instead of just saying \"I'm fine, thanks for asking!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_gemini.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1539f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the student LLM engineer keep a ladder next to their desk?  \n",
       "Because they were training to climb higher‑level abstractions—one layer at a time!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_groq.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cd859",
   "metadata": {},
   "source": [
    "#### Testing model responses with prisoner's dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba7bfb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "dilemma_messag = [{\"role\" : \"user\", \"content\" : dilemma_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f483b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_resp_groq= groq.chat.completions.create(model=\"openai/gpt-oss-120b\",messages=dilemma_messag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48f9fe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I’d choose **Share**. By cooperating, both players walk away with $1,000 each, which is better than the risk of ending up with nothing if both of us decide to steal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(dilemma_resp_groq.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f2fde6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Steal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dilemma_resp_ollama = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=dilemma_messag)\n",
    "display(Markdown(dilemma_resp_ollama.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf0d23",
   "metadata": {},
   "source": [
    "#### Abstractions using LiteLLM and exploring the key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e7e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24235d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_resposne(response):\n",
    "    stream = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in response:\n",
    "        stream += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(stream), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e29b62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their prompt?\n",
       "\n",
       "Because after hours of meticulous crafting and fine-tuning, the prompt still just kept saying, \"I'm sorry, but as an AI language model, I cannot provide relationship advice.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response=completion(model=\"gemini/gemini-2.5-flash\", messages=tell_a_joke, stream=True)\n",
    "stream_resposne(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calling ollama and get stream response\n",
    "resp_ollama = completion(model=\"ollama/gpt-oss:20b\", base_url=\"http://localhost:11434\", messages=tell_a_joke, stream=True)\n",
    "stream_resposne(resp_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ec39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_openai=completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd3dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt tokens : 24\n",
      "output prompt tokens : 36\n",
      "Total tokens : 60\n"
     ]
    }
   ],
   "source": [
    "print(f\"input prompt tokens : {resp_openai.usage.prompt_tokens}\")\n",
    "print(f\"output prompt tokens : {resp_openai.usage.completion_tokens}\")\n",
    "print(f\"Total tokens : {resp_openai.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331aa8f",
   "metadata": {},
   "source": [
    "#### lite llm's prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db39d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "resp_gemini=completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91dbdd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Shakespeare's *Hamlet*, when Laertes returns to Denmark in a rage and demands to know \"Where is my father?\", the reply comes from **Gertrude, the Queen**.\n",
       "\n",
       "She says: **\"One thing to think on.\"**\n",
       "\n",
       "This is a deliberately evasive and unsettling answer, hinting at the turmoil and the hidden truth of Polonius's death without directly revealing it. She's clearly trying to control the situation and perhaps soften the blow of the terrible news."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(resp_gemini.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00bb7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_gemini.usage.prompt_tokens_details.cached_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1afa831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 100\n",
      "Total tokens: 119\n",
      "Cached tokens: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {resp_gemini.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {resp_gemini.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {resp_gemini.usage.total_tokens}\")\n",
    "#print(f\"Total cost: {resp_gemini._hidden_params[\"response_cost\"]*100:.4f} cents\")\n",
    "print(f\"Cached tokens: {resp_gemini.usage.prompt_tokens_details.cached_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd640019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Act II, Scene III, when Laertes asks \"Where is my father?\", the reply comes from Ophelia:\\n\\n**\"My lord, he hath, of late made many tenders / Of his affection to me.\"**\\n\\nHowever, this is not a direct answer to his question about Polonius\\'s whereabouts. Instead, Ophelia is speaking about Hamlet\\'s affection for her, which Laertes had just been warning her about.\\n\\nThe actual location of Polonius is revealed shortly after when he enters the scene.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "question[0][\"content\"] += f\"here is the Hamlet play for the context {hamlet}\"\n",
    "resp_gemini=completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "resp_gemini.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58690ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes bursts into the throne room in Act IV, Scene V, demanding to know where his father is, the Queen's reply is:\n",
       "\n",
       "**\"Alas, my lord, he is dead.\"**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(resp_gemini.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "910f60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53203\n",
      "Output tokens: 108\n",
      "Total tokens: 53311\n",
      "Cached tokens: 52216\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {resp_gemini.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {resp_gemini.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {resp_gemini.usage.total_tokens}\")\n",
    "#print(f\"Total cost: {resp_gemini._hidden_params[\"response_cost\"]*100:.4f} cents\")\n",
    "print(f\"Cached tokens: {resp_gemini.usage.prompt_tokens_details.cached_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
